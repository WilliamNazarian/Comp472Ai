{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83fe88b-e0f5-4a00-86e1-e3a833e8b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.cuda as cuda\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e2ee5e-6dfb-4d16-ab21-55e20657eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_classes = 4\n",
    "learning_rate = 0.001\n",
    "mean_gray = 0.1307\n",
    "stddev_gray = 0.3081\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean_gray,), (stddev_gray,))\n",
    "])\n",
    "\n",
    "# Load your custom dataset\n",
    "dataset_path = r\"C:\\Users\\yasse\\OneDrive\\Desktop\\cleaned_images\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33dd888a-ee0e-4e49-b43d-c4f5a78cad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes = trainset.classes\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681055ad-40db-49a0-a5e7-c4618fbfff73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: neutral\n"
     ]
    }
   ],
   "source": [
    "img = images[12].numpy()  # Get the 13th image in the batch (0-indexed)\n",
    "label = labels[12].item()  # Get the 13th label in the batch\n",
    "\n",
    "# Unnormalize the image for display\n",
    "\n",
    "\n",
    "# Check if the image is color or grayscale\n",
    "if img.shape[0] == 3:  # Color image\n",
    "    img = np.transpose(img, (1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "elif img.shape[0] == 1:  # Grayscale image\n",
    "    img = np.squeeze(img)  # Remove the channel dimension\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img, cmap='gray' if img.ndim == 2 else None)\n",
    "plt.title(f'Label: {classes[label]}')\n",
    "plt.show()\n",
    "\n",
    "# Print the label\n",
    "print(f'Label: {classes[label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f6aa1a4-f94a-4cd8-ac07-67ef3de33828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['anger', 'engaged', 'happy', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate the number of samples for each subset\n",
    "num_train = int(0.7 * len(trainset))  # 70% for training\n",
    "num_val_test = len(trainset) - num_train  # Remaining 30% for validation and testing\n",
    "num_val = int(0.15 * len(trainset))  # 15% for validation\n",
    "num_test = num_val_test - num_val  # 15% for testing\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, temp_dataset = random_split(trainset, [num_train, num_val_test])\n",
    "val_dataset, test_dataset = random_split(temp_dataset, [num_val, num_test])\n",
    "\n",
    "# Create DataLoaders for each subset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Classes: {classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506ca1b-5ac3-4455-8ced-c8909ab09d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d3ab767-c65c-4fd9-b469-44edbce9585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OB_05Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OB_05Model, self).__init__()\n",
    "\n",
    "        # Convolution Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)  # 28 x 28 x 1 -> 24 x 24 x 20\n",
    "        self.relu1 = nn.ReLU()  # Activation function\n",
    "\n",
    "        # Convolution Layer 2\n",
    "        self.conv2 = nn.Conv2d(20, 30, kernel_size=5)  # 24 x 24 x 20 -> 20 x 20 x 30\n",
    "        self.conv2_drop = nn.Dropout2d(p=0.5)  # Dropout\n",
    "        self.maxpool2 = nn.MaxPool2d(2)  # Pooling layer 20 x 20 x 30 -> 10 x 10 x 30\n",
    "        self.relu2 = nn.ReLU()  # Activation function\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(3000, 500)  # 10 x 10 x 30 -> 3000 -> 500\n",
    "        self.fc2 = nn.Linear(500, 10)  # 500 -> 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        # Convolution Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_drop(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eecfe99a-b4a5-4144-87a5-5e9f2cce8f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "model = OB_05Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) \n",
    "dummy_input = torch.randn(32, 1, 28, 28)  # Batch size of 32, 3 channels, 28x28 image\n",
    "output = model(dummy_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "372aaa23-5696-420c-aff7-69519c3d9ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Tr Loss: 1.1968, Tr Acc: 44.7531, Val Loss: 0.7481, Val Acc: 59.6542\n",
      "Epoch 2/100, Tr Loss: 0.7494, Tr Acc: 64.1358, Val Loss: 0.6268, Val Acc: 74.3516\n",
      "Epoch 3/100, Tr Loss: 0.6271, Tr Acc: 73.5802, Val Loss: 0.6116, Val Acc: 74.9280\n",
      "Epoch 4/100, Tr Loss: 0.5938, Tr Acc: 75.7407, Val Loss: 0.5644, Val Acc: 77.8098\n",
      "Epoch 5/100, Tr Loss: 0.5334, Tr Acc: 78.6420, Val Loss: 0.5323, Val Acc: 78.9625\n",
      "Epoch 6/100, Tr Loss: 0.4982, Tr Acc: 79.5062, Val Loss: 0.5085, Val Acc: 81.2680\n",
      "Epoch 7/100, Tr Loss: 0.4527, Tr Acc: 82.2222, Val Loss: 0.5332, Val Acc: 77.8098\n",
      "Epoch 8/100, Tr Loss: 0.4330, Tr Acc: 82.9630, Val Loss: 0.5053, Val Acc: 78.3862\n",
      "Epoch 9/100, Tr Loss: 0.4231, Tr Acc: 84.0741, Val Loss: 0.5314, Val Acc: 78.3862\n",
      "Epoch 10/100, Tr Loss: 0.3425, Tr Acc: 86.7901, Val Loss: 0.5481, Val Acc: 79.2507\n",
      "Epoch 11/100, Tr Loss: 0.3494, Tr Acc: 86.4198, Val Loss: 0.5341, Val Acc: 81.8444\n",
      "Epoch 12/100, Tr Loss: 0.3107, Tr Acc: 88.3951, Val Loss: 0.5509, Val Acc: 79.2507\n",
      "Epoch 13/100, Tr Loss: 0.2875, Tr Acc: 89.0741, Val Loss: 0.5257, Val Acc: 78.3862\n",
      "Epoch 14/100, Tr Loss: 0.2982, Tr Acc: 89.0123, Val Loss: 0.5715, Val Acc: 80.4035\n",
      "Epoch 15/100, Tr Loss: 0.2682, Tr Acc: 90.0617, Val Loss: 0.5049, Val Acc: 81.2680\n",
      "Epoch 16/100, Tr Loss: 0.2606, Tr Acc: 90.3704, Val Loss: 0.5503, Val Acc: 81.2680\n",
      "Epoch 17/100, Tr Loss: 0.2535, Tr Acc: 90.2469, Val Loss: 0.5613, Val Acc: 79.5389\n",
      "Epoch 18/100, Tr Loss: 0.2186, Tr Acc: 92.2222, Val Loss: 0.5566, Val Acc: 83.2853\n",
      "Epoch 19/100, Tr Loss: 0.2381, Tr Acc: 90.7407, Val Loss: 0.5196, Val Acc: 81.8444\n",
      "Epoch 20/100, Tr Loss: 0.1808, Tr Acc: 93.4568, Val Loss: 0.5632, Val Acc: 80.9798\n",
      "Epoch 21/100, Tr Loss: 0.1643, Tr Acc: 93.4568, Val Loss: 0.6149, Val Acc: 79.5389\n",
      "Epoch 22/100, Tr Loss: 0.1811, Tr Acc: 93.1481, Val Loss: 0.6080, Val Acc: 79.8271\n",
      "Epoch 23/100, Tr Loss: 0.1590, Tr Acc: 93.2716, Val Loss: 0.6230, Val Acc: 81.2680\n",
      "Epoch 24/100, Tr Loss: 0.1521, Tr Acc: 94.0123, Val Loss: 0.6660, Val Acc: 79.8271\n",
      "Epoch 25/100, Tr Loss: 0.1543, Tr Acc: 93.8889, Val Loss: 0.6603, Val Acc: 81.5562\n",
      "Epoch 26/100, Tr Loss: 0.1415, Tr Acc: 93.9506, Val Loss: 0.6847, Val Acc: 80.6916\n",
      "Epoch 27/100, Tr Loss: 0.1255, Tr Acc: 94.5679, Val Loss: 0.6714, Val Acc: 82.4207\n",
      "Epoch 28/100, Tr Loss: 0.1198, Tr Acc: 96.1111, Val Loss: 0.6290, Val Acc: 81.8444\n",
      "Epoch 29/100, Tr Loss: 0.0995, Tr Acc: 96.3580, Val Loss: 0.7084, Val Acc: 81.2680\n",
      "Epoch 30/100, Tr Loss: 0.0979, Tr Acc: 96.1111, Val Loss: 0.7146, Val Acc: 80.9798\n",
      "Epoch 31/100, Tr Loss: 0.1138, Tr Acc: 95.7407, Val Loss: 0.6559, Val Acc: 81.8444\n",
      "Epoch 32/100, Tr Loss: 0.0853, Tr Acc: 97.0370, Val Loss: 0.7437, Val Acc: 81.5562\n",
      "Epoch 33/100, Tr Loss: 0.0699, Tr Acc: 97.8395, Val Loss: 0.7982, Val Acc: 81.2680\n",
      "Epoch 34/100, Tr Loss: 0.0921, Tr Acc: 96.4198, Val Loss: 0.7782, Val Acc: 81.8444\n",
      "Epoch 35/100, Tr Loss: 0.0846, Tr Acc: 96.7901, Val Loss: 0.7443, Val Acc: 80.9798\n",
      "Epoch 36/100, Tr Loss: 0.1040, Tr Acc: 96.4815, Val Loss: 0.7407, Val Acc: 80.1153\n",
      "Epoch 37/100, Tr Loss: 0.0790, Tr Acc: 97.6543, Val Loss: 0.7618, Val Acc: 80.9798\n",
      "Epoch 38/100, Tr Loss: 0.0669, Tr Acc: 97.6543, Val Loss: 0.8128, Val Acc: 80.4035\n",
      "Epoch 39/100, Tr Loss: 0.0789, Tr Acc: 96.8519, Val Loss: 0.8084, Val Acc: 80.1153\n",
      "Epoch 40/100, Tr Loss: 0.0575, Tr Acc: 98.2716, Val Loss: 0.8614, Val Acc: 81.8444\n",
      "Epoch 41/100, Tr Loss: 0.0480, Tr Acc: 98.5802, Val Loss: 0.8963, Val Acc: 81.8444\n",
      "Epoch 42/100, Tr Loss: 0.0578, Tr Acc: 97.8395, Val Loss: 0.8534, Val Acc: 81.2680\n",
      "Epoch 43/100, Tr Loss: 0.0779, Tr Acc: 97.1605, Val Loss: 0.8978, Val Acc: 81.5562\n",
      "Epoch 44/100, Tr Loss: 0.0722, Tr Acc: 97.2222, Val Loss: 0.8858, Val Acc: 81.5562\n",
      "Epoch 45/100, Tr Loss: 0.0462, Tr Acc: 98.3333, Val Loss: 0.8358, Val Acc: 80.6916\n",
      "Epoch 46/100, Tr Loss: 0.0663, Tr Acc: 97.4074, Val Loss: 0.7966, Val Acc: 80.4035\n",
      "Epoch 47/100, Tr Loss: 0.0610, Tr Acc: 97.6543, Val Loss: 0.8745, Val Acc: 80.4035\n",
      "Epoch 48/100, Tr Loss: 0.0496, Tr Acc: 98.3951, Val Loss: 0.7995, Val Acc: 80.6916\n",
      "Epoch 49/100, Tr Loss: 0.0427, Tr Acc: 98.3951, Val Loss: 0.8227, Val Acc: 82.7089\n",
      "Epoch 50/100, Tr Loss: 0.0809, Tr Acc: 96.9136, Val Loss: 0.8738, Val Acc: 82.1326\n",
      "Epoch 51/100, Tr Loss: 0.0719, Tr Acc: 97.2222, Val Loss: 0.8158, Val Acc: 80.9798\n",
      "Epoch 52/100, Tr Loss: 0.0501, Tr Acc: 98.2099, Val Loss: 0.9030, Val Acc: 80.4035\n",
      "Epoch 53/100, Tr Loss: 0.0546, Tr Acc: 98.2099, Val Loss: 0.8835, Val Acc: 82.4207\n",
      "Epoch 54/100, Tr Loss: 0.0302, Tr Acc: 98.8272, Val Loss: 0.8860, Val Acc: 82.4207\n",
      "Epoch 55/100, Tr Loss: 0.0374, Tr Acc: 98.4568, Val Loss: 0.9378, Val Acc: 82.1326\n",
      "Epoch 56/100, Tr Loss: 0.0511, Tr Acc: 98.3333, Val Loss: 1.0753, Val Acc: 81.2680\n",
      "Epoch 57/100, Tr Loss: 0.0565, Tr Acc: 97.9012, Val Loss: 1.0690, Val Acc: 80.6916\n",
      "Epoch 58/100, Tr Loss: 0.0589, Tr Acc: 97.7160, Val Loss: 0.9518, Val Acc: 81.8444\n",
      "Epoch 59/100, Tr Loss: 0.0666, Tr Acc: 97.5309, Val Loss: 1.0762, Val Acc: 81.5562\n",
      "Epoch 60/100, Tr Loss: 0.0626, Tr Acc: 97.8395, Val Loss: 0.9522, Val Acc: 81.2680\n",
      "Epoch 61/100, Tr Loss: 0.0450, Tr Acc: 98.2099, Val Loss: 1.0051, Val Acc: 80.6916\n",
      "Epoch 62/100, Tr Loss: 0.0505, Tr Acc: 97.9012, Val Loss: 0.8991, Val Acc: 82.7089\n",
      "Epoch 63/100, Tr Loss: 0.0527, Tr Acc: 97.7778, Val Loss: 0.8410, Val Acc: 81.5562\n",
      "Epoch 64/100, Tr Loss: 0.0746, Tr Acc: 97.4691, Val Loss: 0.8800, Val Acc: 81.5562\n",
      "Epoch 65/100, Tr Loss: 0.0418, Tr Acc: 98.6420, Val Loss: 0.9575, Val Acc: 81.5562\n",
      "Epoch 66/100, Tr Loss: 0.0275, Tr Acc: 99.1975, Val Loss: 1.1180, Val Acc: 80.9798\n",
      "Epoch 67/100, Tr Loss: 0.0496, Tr Acc: 98.6420, Val Loss: 1.0291, Val Acc: 81.2680\n",
      "Epoch 68/100, Tr Loss: 0.0369, Tr Acc: 98.7654, Val Loss: 1.0896, Val Acc: 82.7089\n",
      "Epoch 69/100, Tr Loss: 0.0815, Tr Acc: 96.9136, Val Loss: 0.7877, Val Acc: 81.8444\n",
      "Epoch 70/100, Tr Loss: 0.0462, Tr Acc: 98.0247, Val Loss: 0.9906, Val Acc: 79.8271\n",
      "Epoch 71/100, Tr Loss: 0.0318, Tr Acc: 98.6420, Val Loss: 0.9237, Val Acc: 81.8444\n",
      "Epoch 72/100, Tr Loss: 0.0384, Tr Acc: 98.4568, Val Loss: 0.9545, Val Acc: 80.6916\n",
      "Epoch 73/100, Tr Loss: 0.0365, Tr Acc: 98.7654, Val Loss: 1.0787, Val Acc: 80.9798\n",
      "Epoch 74/100, Tr Loss: 0.0271, Tr Acc: 99.1975, Val Loss: 1.0799, Val Acc: 80.9798\n",
      "Epoch 75/100, Tr Loss: 0.0334, Tr Acc: 98.8889, Val Loss: 1.1522, Val Acc: 81.8444\n",
      "Epoch 76/100, Tr Loss: 0.0374, Tr Acc: 98.8889, Val Loss: 1.0544, Val Acc: 81.2680\n",
      "Epoch 77/100, Tr Loss: 0.0285, Tr Acc: 98.8272, Val Loss: 1.0343, Val Acc: 82.4207\n",
      "Epoch 78/100, Tr Loss: 0.0305, Tr Acc: 99.1358, Val Loss: 1.1112, Val Acc: 82.1326\n",
      "Epoch 79/100, Tr Loss: 0.0438, Tr Acc: 97.9630, Val Loss: 1.1871, Val Acc: 81.5562\n",
      "Epoch 80/100, Tr Loss: 0.0489, Tr Acc: 98.2716, Val Loss: 1.1528, Val Acc: 80.9798\n",
      "Epoch 81/100, Tr Loss: 0.0300, Tr Acc: 98.7654, Val Loss: 1.1599, Val Acc: 81.2680\n",
      "Epoch 82/100, Tr Loss: 0.0333, Tr Acc: 98.8889, Val Loss: 1.1355, Val Acc: 79.8271\n",
      "Epoch 83/100, Tr Loss: 0.0499, Tr Acc: 98.3333, Val Loss: 1.1458, Val Acc: 80.6916\n",
      "Epoch 84/100, Tr Loss: 0.0451, Tr Acc: 98.4568, Val Loss: 1.1008, Val Acc: 81.2680\n",
      "Epoch 85/100, Tr Loss: 0.0385, Tr Acc: 98.7654, Val Loss: 1.1758, Val Acc: 80.1153\n",
      "Epoch 86/100, Tr Loss: 0.0347, Tr Acc: 99.0123, Val Loss: 1.1247, Val Acc: 81.2680\n",
      "Epoch 87/100, Tr Loss: 0.0338, Tr Acc: 98.7654, Val Loss: 1.2334, Val Acc: 80.1153\n",
      "Epoch 88/100, Tr Loss: 0.0259, Tr Acc: 99.0123, Val Loss: 1.1134, Val Acc: 81.8444\n",
      "Epoch 89/100, Tr Loss: 0.0411, Tr Acc: 98.5185, Val Loss: 1.2047, Val Acc: 82.4207\n",
      "Epoch 90/100, Tr Loss: 0.0387, Tr Acc: 98.9506, Val Loss: 1.1608, Val Acc: 80.9798\n",
      "Epoch 91/100, Tr Loss: 0.0566, Tr Acc: 97.8395, Val Loss: 1.0334, Val Acc: 80.9798\n",
      "Epoch 92/100, Tr Loss: 0.0434, Tr Acc: 98.7037, Val Loss: 1.0438, Val Acc: 80.4035\n",
      "Epoch 93/100, Tr Loss: 0.0520, Tr Acc: 98.2099, Val Loss: 0.9744, Val Acc: 81.2680\n",
      "Epoch 94/100, Tr Loss: 0.0347, Tr Acc: 99.0741, Val Loss: 0.9595, Val Acc: 81.8444\n",
      "Epoch 95/100, Tr Loss: 0.0316, Tr Acc: 98.6420, Val Loss: 1.0848, Val Acc: 81.2680\n",
      "Epoch 96/100, Tr Loss: 0.0440, Tr Acc: 98.4568, Val Loss: 1.0537, Val Acc: 80.4035\n",
      "Epoch 97/100, Tr Loss: 0.0524, Tr Acc: 98.0247, Val Loss: 1.0578, Val Acc: 80.4035\n",
      "Epoch 98/100, Tr Loss: 0.0177, Tr Acc: 99.3210, Val Loss: 1.1622, Val Acc: 79.2507\n",
      "Epoch 99/100, Tr Loss: 0.0293, Tr Acc: 98.8889, Val Loss: 1.2006, Val Acc: 80.4035\n",
      "Epoch 100/100, Tr Loss: 0.0263, Tr Acc: 98.8272, Val Loss: 1.1716, Val Acc: 80.1153\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ############################\n",
    "    # Train\n",
    "    ############################\n",
    "    \n",
    "    iter_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    model.train()  # Put the network into training mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(train_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear off the gradients from any past operation\n",
    "        outputs = model(items)  # Do the forward pass\n",
    "        loss = criterion(outputs, classes)  # Calculate the loss\n",
    "        iter_loss += loss.item()  # Accumulate the loss\n",
    "        loss.backward()  # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()  # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum().item()\n",
    "        iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss / iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append((100 * correct / len(train_loader.dataset)))\n",
    "   \n",
    "\n",
    "    ############################\n",
    "    # Validate - How did we do on the unseen dataset?\n",
    "    ############################\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()  # Put the network into evaluate mode\n",
    "    \n",
    "    for i, (items, classes) in enumerate(val_loader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            items = items.cuda()\n",
    "            classes = classes.cuda()\n",
    "        \n",
    "        outputs = model(items)  # Do the forward pass\n",
    "        loss += criterion(outputs, classes).item()  # Calculate the loss\n",
    "        \n",
    "        # Record the correct predictions for validation data\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == classes.data).sum().item()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the validation loss\n",
    "    valid_loss.append(loss / iterations)\n",
    "    # Record the validation accuracy\n",
    "    valid_accuracy.append(correct / len(val_loader.dataset) * 100.0)\n",
    "\n",
    "    print('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f'\n",
    "          % (epoch + 1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             valid_loss[-1], valid_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b1acb-1d31-4b75-8e99-6a718881a72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "928b5b4b-d020-4156-91e2-0da6d9526e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on  test images: 79.31 %\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to track correct predictions and total samples\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Update total number of labels\n",
    "        total += labels.size(0)\n",
    "        # Update the number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = (correct / total) * 100\n",
    "print('Test Accuracy of the model on  test images: {:.2f} %'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfb0fbf6-1fe7-451b-a10e-714955e64a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of the model: 81.56 %\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to track correct predictions and total samples\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Update total number of labels\n",
    "        total += labels.size(0)\n",
    "        # Update the number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = (correct / total) * 100\n",
    "print('Validation Accuracy of the model: {:.2f} %'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef357d6-3b63-4995-adf0-a6b76f6a5bb1",
   "metadata": {},
   "source": [
    "Save the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7518c471-6a59-4a8e-823c-fc4c2ec58cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to C:\\Users\\yasse\\OneDrive\\Desktop\\ModelPath\\model1.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the path and file name to save the model\n",
    "model_path = r\"C:\\Users\\yasse\\OneDrive\\Desktop\\ModelPath\\model1.pth\"\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f15d2-2c0f-463c-92a6-2203d3f315db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
