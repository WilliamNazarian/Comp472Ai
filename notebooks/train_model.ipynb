{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "4454a851b11ab2e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T21:39:03.942727Z",
     "start_time": "2024-06-11T21:39:03.938221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import scripts.data_loader as data_loader\n",
    "import src.training as training\n",
    "import src.evaluation as evaluation\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from src.types import * \n",
    "from src.models.main_model import OB_05Model\n",
    "from src.models.main_model_v1 import OB_05Model_Variant1\n",
    "from src.models.main_model_v2 import OB_05Model_Variant2 "
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "68eb269ba45a0e22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_dataset, validation_dataset, testing_dataset = data_loader.split_images_dataset()\n",
    "torch.save(training_dataset, \"models/training_dataset.pth\")\n",
    "torch.save(validation_dataset, \"models/validation_dataset.pth\")\n",
    "torch.save(testing_dataset, \"models/testing_dataset.pth\")\n",
    "\n",
    "training_set_loader = data_loader.create_data_loader(training_dataset)\n",
    "validation_set_loader = data_loader.create_data_loader(validation_dataset)\n",
    "testing_set_loader = data_loader.create_data_loader(testing_dataset)"
   ],
   "id": "f1797530dca4d0b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = OB_05Model()\n",
    "initial_learning_rate = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_learning_rate, weight_decay=5e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
    "\n",
    "training_config = training.TrainingConfig(\n",
    "    output_dir=r\"models/\",\n",
    "    \n",
    "    training_set_loader=training_set_loader,\n",
    "    validation_set_loader=validation_set_loader,\n",
    "    testing_set_loader=testing_set_loader,\n",
    "\n",
    "    epochs=100,\n",
    "\n",
    "    classes=data_loader.get_trainset().classes,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler\n",
    ")"
   ],
   "id": "ca0a431c1eb1fb45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_logger = training.train_model(training_config)\n",
    "\n",
    "with open(\"models/training_logger.pkl\", \"wb\") as file:\n",
    "    pickle.dump(training_logger, file)\n",
    "\n",
    "torch.save(model.state_dict(), 'models/model.pth')"
   ],
   "id": "1c623c5fb0744988",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# For the variant #1",
   "id": "2800e89bfdf07ed2"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-11T21:39:08.130341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset, validation_dataset, testing_dataset = data_loader.split_images_dataset()\n",
    "torch.save(training_dataset, \"model_v1/training_dataset.pth\")\n",
    "torch.save(validation_dataset, \"model_v1/validation_dataset.pth\")\n",
    "torch.save(testing_dataset, \"model_v1/testing_dataset.pth\")\n",
    "\n",
    "training_set_loader = data_loader.create_data_loader(training_dataset)\n",
    "validation_set_loader = data_loader.create_data_loader(validation_dataset)\n",
    "testing_set_loader = data_loader.create_data_loader(testing_dataset)\n",
    "\n",
    "model = OB_05Model_Variant1()\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_learning_rate, weight_decay=5e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
    "\n",
    "training_config = training.TrainingConfig(\n",
    "    output_dir=r\"model_v1/\",\n",
    "\n",
    "    training_set_loader=training_set_loader,\n",
    "    validation_set_loader=validation_set_loader,\n",
    "    testing_set_loader=testing_set_loader,\n",
    "\n",
    "    epochs=100,\n",
    "\n",
    "    classes=data_loader.get_trainset().classes,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "model.apply(weights_init)\n",
    "\n",
    "training_logger = training.train_model(training_config)\n",
    "\n",
    "with open(\"model_v1/training_logger.pkl\", \"wb\") as file:\n",
    "    pickle.dump(training_logger, file)\n",
    "\n",
    "torch.save(model.state_dict(), 'model_v1/model.pth')"
   ],
   "id": "5757878946d7f797",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "\tTraining precision: 0.4345\n",
      "\tTraining recall: 0.4345\n",
      "\tTraining accuracy: 0.7194\n",
      "\tTraining f1-score: 0.4340\n",
      "\n",
      "\tValidation precision: 0.6369\n",
      "\tValidation recall: 0.5672\n",
      "\tValidation accuracy: 0.7795\n",
      "\tValidation f1-score: 0.5349\n",
      "\tLearning rate: 0.0001\n",
      "\n",
      "\n",
      "Epoch 2/100:\n",
      "\tTraining precision: 0.5539\n",
      "\tTraining recall: 0.5536\n",
      "\tTraining accuracy: 0.7784\n",
      "\tTraining f1-score: 0.5537\n",
      "\n",
      "\tValidation precision: 0.7605\n",
      "\tValidation recall: 0.5665\n",
      "\tValidation accuracy: 0.7997\n",
      "\tValidation f1-score: 0.5022\n",
      "\tLearning rate: 0.0001\n",
      "\n",
      "\n",
      "Epoch 3/100:\n",
      "\tTraining precision: 0.6017\n",
      "\tTraining recall: 0.6001\n",
      "\tTraining accuracy: 0.8019\n",
      "\tTraining f1-score: 0.6003\n",
      "\n",
      "\tValidation precision: 0.6650\n",
      "\tValidation recall: 0.6487\n",
      "\tValidation accuracy: 0.8199\n",
      "\tValidation f1-score: 0.6294\n",
      "\tLearning rate: 0.0001\n",
      "\n",
      "\n",
      "Epoch 4/100:\n",
      "\tTraining precision: 0.6001\n",
      "\tTraining recall: 0.5990\n",
      "\tTraining accuracy: 0.8019\n",
      "\tTraining f1-score: 0.5989\n",
      "\n",
      "\tValidation precision: 0.7222\n",
      "\tValidation recall: 0.6412\n",
      "\tValidation accuracy: 0.8300\n",
      "\tValidation f1-score: 0.6189\n",
      "\tLearning rate: 0.0001\n",
      "\n",
      "\n",
      "Epoch 5/100:\n",
      "\tTraining precision: 0.5933\n",
      "\tTraining recall: 0.5931\n",
      "\tTraining accuracy: 0.7988\n",
      "\tTraining f1-score: 0.5931\n",
      "\n",
      "\tValidation precision: 0.6740\n",
      "\tValidation recall: 0.6377\n",
      "\tValidation accuracy: 0.8329\n",
      "\tValidation f1-score: 0.6160\n",
      "\tLearning rate: 0.0001\n",
      "\n",
      "\n",
      "Epoch 6/100:\n",
      "\tTraining precision: 0.6332\n",
      "\tTraining recall: 0.6316\n",
      "\tTraining accuracy: 0.8173\n",
      "\tTraining f1-score: 0.6319\n",
      "\n",
      "\tValidation precision: 0.6543\n",
      "\tValidation recall: 0.6349\n",
      "\tValidation accuracy: 0.8314\n",
      "\tValidation f1-score: 0.6117\n",
      "\tLearning rate: 0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset, validation_dataset, testing_dataset = data_loader.split_images_dataset()\n",
    "torch.save(training_dataset, \"model_v2/training_dataset.pth\")\n",
    "torch.save(validation_dataset, \"model_v2/validation_dataset.pth\")\n",
    "torch.save(testing_dataset, \"model_v2/testing_dataset.pth\")\n",
    "\n",
    "training_set_loader = data_loader.create_data_loader(training_dataset)\n",
    "validation_set_loader = data_loader.create_data_loader(validation_dataset)\n",
    "testing_set_loader = data_loader.create_data_loader(testing_dataset)\n",
    "\n",
    "model = OB_05Model_Variant2()\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_learning_rate, weight_decay=5e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
    "\n",
    "training_config = training.TrainingConfig(\n",
    "    models_output_dir=r\"model_v2/\",\n",
    "\n",
    "    training_set_loader=training_set_loader,\n",
    "    validation_set_loader=validation_set_loader,\n",
    "    testing_set_loader=testing_set_loader,\n",
    "\n",
    "    epochs=100,\n",
    "\n",
    "    classes=data_loader.get_trainset().classes,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "model.apply(weights_init)\n",
    "\n",
    "training_logger = training.train_model(training_config)\n",
    "\n",
    "with open(\"model_v2/training_logger.pkl\", \"wb\") as file:\n",
    "    pickle.dump(training_logger, file)\n",
    "\n",
    "torch.save(model.state_dict(), 'model_v2/model.pth')\n"
   ],
   "id": "7db4a674d6caab3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "13542d83f2e89801"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
