{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following notebook demonstrates the main steps of the pipeline:\n",
    "1. Setup (Defining the model and parameters)\n",
    "2. Training the model\n",
    "3. Testing the model\n",
    "\n",
    "At its core, the `main.py` module does the exact same thing here with extra steps (for file and user handling)."
   ],
   "id": "69b1ce2bfa5e6c23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "First, we need to setup the training configuration. This includes defining the training/validation/testing datasets, defining the model, number of epochs, the optimiser/loss function, etc. All these can be neatly packed into a single object called [TrainingConfig](../src/types.py). "
   ],
   "id": "17e3285d96f30c0e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import scripts.data_loader as data_loader\n",
    "import src.training as training\n",
    "import src.evaluation as evaluation\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from src.types import *\n",
    "from src.models.main_model import OB_05Model\n",
    "from src.models.main_model_v1 import OB_05Model_Variant1\n",
    "from src.models.main_model_v2 import OB_05Model_Variant2\n",
    "from scripts.visualization.model_evaluation import TrainingVisualizations, TestingVisualizations\n",
    "\n",
    "\n",
    "output_dir = r\"../output/pipeline_demo/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "# Initialize datasets\n",
    "training_dataset, validation_dataset, testing_dataset = data_loader.split_images_dataset()\n",
    "\n",
    "training_set_loader = data_loader.create_data_loader(training_dataset)\n",
    "validation_set_loader = data_loader.create_data_loader(validation_dataset)\n",
    "testing_set_loader = data_loader.create_data_loader(testing_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# logger for output (we can output training data to stdout or a file for example)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# can pick any model\n",
    "model = OB_05Model()\n",
    "# model = OB_05Model_Variant1()\n",
    "# model = OB_05Model_Variant2()\n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_learning_rate, weight_decay=5e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
    "\n",
    "training_config = training.TrainingConfig(\n",
    "    model_name=\"pipeline_demo\",\n",
    "    output_dir=output_dir,\n",
    "    output_logger=logger,\n",
    "\n",
    "    training_set_loader=training_set_loader,\n",
    "    validation_set_loader=validation_set_loader,\n",
    "    testing_set_loader=testing_set_loader,\n",
    "\n",
    "    epochs=10,\n",
    "\n",
    "    classes=data_loader.get_trainset().classes,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler\n",
    ")"
   ],
   "id": "aeb542ed49db37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training &amp; Visualizations",
   "id": "4da0b57a4df1ae3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_logger = training.train_model(training_config)\n",
    "\n",
    "# save the training logger -> we can visualize data without having to re-train the model\n",
    "training_logger_path = os.path.join(output_dir, \"training_logger.pkl\")\n",
    "with open(training_logger_path, \"wb\") as file:\n",
    "    pickle.dump(training_logger, file)\n",
    "\n",
    "# save the model so we can test it without having to re-train the model\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"model.pth\"))"
   ],
   "id": "8dd6ab2367c67ab1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualization functions are located in the [TrainingVisualizations](../scripts/visualization/model_evaluation.py) script/module.",
   "id": "934d96dc47a8a06c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fig = TrainingVisualizations.plot_training_metrics(training_logger)",
   "id": "46bf43e3585503e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing & Visualizations",
   "id": "32c574a2a9bd2d9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluation_results = evaluation.evaluate_model(logger, model, testing_set_loader)",
   "id": "d201eedd71c2ded0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualization functions are located in the [TestingVisualizations](../scripts/visualization/model_evaluation.py) script/module.\n",
   "id": "52880bb5ddcdb852"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generated confusion matrix from the test set\n",
    "_ = TestingVisualizations.generate_confusion_matrix_table(evaluation_results)"
   ],
   "id": "79b3392c701910d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Overall metrics table from the test set\n",
    "# Displays macro and micro metrics\n",
    "_ = TestingVisualizations.generate_overall_metrics_table(evaluation_results)"
   ],
   "id": "fe968a15541e5858",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Displays the metrics per emotion class AS A TABLE\n",
    "_ = TestingVisualizations.generate_metrics_per_class_table(evaluation_results)"
   ],
   "id": "88b33f5491ff2c27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Displays the metrics per emotion class AS A BAR PLOT\n",
    "_ = TestingVisualizations.plot_metrics_per_class(evaluation_results)"
   ],
   "id": "7b7949f89c289e9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Additional functionalities\n",
    "Metrics are calculated using a confusion table. Since we have 4 classes (angry, engaged, happy, neutral), our confusion matrix is a 4x4 matrix. For information on how metrics are calculated, see [this](https://akash-borgalli.medium.com/confusion-matrix-for-n-x-n-matrix-488e8ff18321) article.\n",
    "\n",
    "The [ConfusionMatrix](../src/utils/confusion_matrix.py) class contains the methods for calculating not only the overall metrics for the model, but the metrics for each individual class. If you want to perform extra processing or visualizations on the `EvaluationResults` objects returned from testing, there are some pre-defined methods that you can use as seen below:"
   ],
   "id": "efd5fa59fdb6a418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluation_results.get_metrics_per_class_as_df()",
   "id": "4d1135aef36aef65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluation_results.get_confusion_matrix_as_df()",
   "id": "220f0d9574c2748d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluation_results.get_metrics_table_as_df()",
   "id": "25fe2f7023b279fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can format the above dataframe into a better visualization as seen below. Note that you can only do this in Jupyter notebooks (can't do it in the console)",
   "id": "303c037713dd117f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "confusion_matrix = evaluation_results.confusion_matrix\n",
    "\n",
    "macro_precision, macro_recall, macro_f1_score, macro_accuracy = cm_macro.calculate_overall_metrics(confusion_matrix)\n",
    "micro_precision, micro_recall, micro_f1_score, micro_accuracy = cm_micro.calculate_overall_metrics(confusion_matrix)\n",
    "data = [[macro_precision, macro_recall, macro_f1_score, micro_precision, micro_recall, micro_f1_score, (macro_accuracy + micro_accuracy)]]\n",
    "tuples = [(\"macro\", \"precision\"), (\"macro\", \"recall\"), (\"macro\", \"f1_score\"), (\"micro\", \"precision\"), (\"micro\", \"recall\"), (\"micro\", \"f1_score\"), (\"\", \"accuracy\")]\n",
    "\n",
    "df = pd.DataFrame(data, index=pd.Index([\"model\"]), columns=pd.MultiIndex.from_tuples(tuples, names=[\"\", \"metrics\"]))\n",
    "df.style"
   ],
   "id": "8105f988de49c58e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Changing the training/testing\n",
    "All models are trained and tested the same way for more accurate comparisons between them. The training portion of the application happens in the [training.py](../src/training.py) module, while the testing/evaluation part happens in the [evaluation.py](../src/evaluation.py) module.\n",
    "\n",
    "### Adding models\n",
    "You can define your own models in the `src/models` directory. You can then train and test these models by placing them in the training config."
   ],
   "id": "a557e08d87b06643"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "End",
   "id": "535139b7012a06f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
