{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hx7Cf5mPNIyT",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git init\n",
        "\n",
        "# Uses (restricted) GitHub token to access private repo\n",
        "# Valid for 30 days starting 6/15/2024\n",
        "!git remote add origin https://bryjen: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@github.com/WilliamNazarian/Comp472Ai.git\n",
        "!git fetch origin\n",
        "!git reset --hard origin/main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt\n",
        "!pip install pipe"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rniT-ocVRYBd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import pickle\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import src.training as training\n",
        "import src.evaluation as evaluation\n",
        "import scripts.data_loader as data_loader\n",
        "\n",
        "\n",
        "from dataclasses import dataclass, asdict\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "from src.types import *\n",
        "from src.models.main_model import OB_05Model\n",
        "from src.models.main_model_v1 import OB_05Model_Variant1\n",
        "from src.models.main_model_v2 import OB_05Model_Variant2\n",
        "from scripts.visualization.model_evaluation import TrainingVisualizations, TestingVisualizations"
      ],
      "metadata": {
        "id": "e7DNRRywTeW7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Project code adjusted so it can run on colab\n",
        "# TODO: Find a way to auto-generate this with modifications\n",
        "#       OR find a way to determine if running on colab vs local\n",
        "\n",
        "__transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "def get_trainset_colab(use_colored=False):\n",
        "    return datasets.ImageFolder(root=\"part1/cleaned_images\", transform=__transform)\n",
        "\n",
        "# Splits the dataset to training, validation, and test sub-datasets\n",
        "def split_images_dataset_colab(use_colored=False):\n",
        "    # images_directory = __colored_images_directory if use_colored else __greyscale_images_directory\n",
        "    trainset = datasets.ImageFolder(root=\"part1/cleaned_images\", transform=__transform)\n",
        "\n",
        "    training_ratio = 0.7  # x% of the dataset is for training\n",
        "    validation_ratio = 0.15  # y% of the dataset is for validation\n",
        "    # (1 - x - y)% for testing\n",
        "\n",
        "    # calculating the number of images per dataset partition\n",
        "    training_set_length = int(training_ratio * len(trainset))\n",
        "    validation_set_length = int(validation_ratio * len(trainset))\n",
        "    testing_set_length = len(trainset) - training_set_length - validation_set_length\n",
        "\n",
        "    # splitting the datasets\n",
        "    lengths = [training_set_length, validation_set_length + testing_set_length]\n",
        "    training_dataset, validation_and_testing_dataset = random_split(trainset, lengths)\n",
        "\n",
        "    lengths = [validation_set_length, testing_set_length]\n",
        "    validation_dataset, testing_dataset = random_split(validation_and_testing_dataset, lengths)\n",
        "\n",
        "    return training_dataset, validation_dataset, testing_dataset"
      ],
      "metadata": {
        "id": "arteCw5eWbQR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix\n",
        "cm_macro = ConfusionMatrix.Macro\n",
        "cm_micro = ConfusionMatrix.Micro\n",
        "\n",
        "output_dir = r\"output/pipeline_demo/\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "\n",
        "# Initialize datasets\n",
        "training_dataset, validation_dataset, testing_dataset = split_images_dataset_colab()\n",
        "\n",
        "training_set_loader = DataLoader(training_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "validation_set_loader = DataLoader(validation_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "testing_set_loader = DataLoader(testing_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "kUGGQZCNSvL8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "o5agODD2dZxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logger for output (we can output training data to stdout or a file for example)\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# can pick any model\n",
        "model = OB_05Model()\n",
        "# model = OB_05Model_Variant1()\n",
        "# model = OB_05Model_Variant2()\n",
        "\n",
        "initial_learning_rate = 0.0001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=initial_learning_rate, weight_decay=5e-2)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
        "\n",
        "training_config = training.TrainingConfig(\n",
        "    model_name=\"pipeline_demo\",\n",
        "    output_dir=output_dir,\n",
        "    output_logger=logger,\n",
        "\n",
        "    training_set_loader=training_set_loader,\n",
        "    validation_set_loader=validation_set_loader,\n",
        "    testing_set_loader=testing_set_loader,\n",
        "\n",
        "    epochs=100,\n",
        "\n",
        "    classes=get_trainset_colab().classes,\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler\n",
        ")"
      ],
      "metadata": {
        "id": "fqGxmaM3TWSG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "YP49IilKdWXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_logger = training.train_model(training_config)\n",
        "\n",
        "# save the model so we can test it without having to re-train the model\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"model.pth\"))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "irf8a3elTW7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = TrainingVisualizations.plot_training_metrics(training_logger)"
      ],
      "metadata": {
        "id": "ni30RtH7dLzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "OnRyWJpldUX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = evaluation.evaluate_model(logger, model, testing_set_loader)"
      ],
      "metadata": {
        "id": "9kv5L83ZdPt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix = evaluation_results.confusion_matrix\n",
        "\n",
        "macro_precision, macro_recall, macro_f1_score, macro_accuracy = cm_macro.calculate_overall_metrics(confusion_matrix)\n",
        "micro_precision, micro_recall, micro_f1_score, micro_accuracy = cm_micro.calculate_overall_metrics(confusion_matrix)\n",
        "data = [[macro_precision, macro_recall, macro_f1_score, micro_precision, micro_recall, micro_f1_score, (macro_accuracy + micro_accuracy)]]\n",
        "tuples = [(\"macro\", \"precision\"), (\"macro\", \"recall\"), (\"macro\", \"f1_score\"), (\"micro\", \"precision\"), (\"micro\", \"recall\"), (\"micro\", \"f1_score\"), (\"\", \"accuracy\")]\n",
        "\n",
        "df = pd.DataFrame(data, index=pd.Index([\"model\"]), columns=pd.MultiIndex.from_tuples(tuples, names=[\"\", \"metrics\"]))\n",
        "df.style"
      ],
      "metadata": {
        "id": "tc2MxhfUdrIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = TestingVisualizations.generate_confusion_matrix_table(evaluation_results)\n",
        "\n",
        "_ = TestingVisualizations.generate_metrics_per_class_table(evaluation_results)\n",
        "\n",
        "_ = TestingVisualizations.plot_metrics_per_class(evaluation_results)"
      ],
      "metadata": {
        "id": "dsgshOL7dTwp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}