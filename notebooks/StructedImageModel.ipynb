{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c83fe88b-e0f5-4a00-86e1-e3a833e8b9dd",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06e2ee5e-6dfb-4d16-ab21-55e20657eff7",
   "metadata": {},
   "source": [
    "num_epochs = 20\n",
    "num_classes = 4\n",
    "learning_rate = 0.001\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "dataset_path = r\"C:\\Users\\yasse\\OneDrive\\Desktop\\structured_data\"\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33dd888a-ee0e-4e49-b43d-c4f5a78cad07",
   "metadata": {},
   "source": [
    "trainset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes = trainset.classes\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "681055ad-40db-49a0-a5e7-c4618fbfff73",
   "metadata": {},
   "source": [
    "img = images[12].numpy()  # Get the 13th image in the batch (0-indexed)\n",
    "label = labels[12].item()  # Get the 13th label in the batch\n",
    "\n",
    "# Unnormalize the image for display\n",
    "\n",
    "\n",
    "# Check if the image is color or grayscale\n",
    "if img.shape[0] == 3:  # Color image\n",
    "    img = np.transpose(img, (1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "elif img.shape[0] == 1:  # Grayscale image\n",
    "    img = np.squeeze(img)  # Remove the channel dimension\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img, cmap='gray' if img.ndim == 2 else None)\n",
    "plt.title(f'Label: {classes[label]}')\n",
    "plt.show()\n",
    "\n",
    "# Print the label\n",
    "print(f'Label: {classes[label]}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f6aa1a4-f94a-4cd8-ac07-67ef3de33828",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Calculate the number of samples for each subset\n",
    "num_train = int(0.7 * len(trainset))  # 70% for training\n",
    "num_val_test = len(trainset) - num_train  # Remaining 30% for validation and testing\n",
    "num_val = int(0.15 * len(trainset))  # 15% for validation\n",
    "num_test = num_val_test - num_val  # 15% for testing\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, temp_dataset = random_split(trainset, [num_train, num_val_test])\n",
    "val_dataset, test_dataset = random_split(temp_dataset, [num_val, num_test])\n",
    "\n",
    "# Create DataLoaders for each subset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Classes: {classes}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506ca1b-5ac3-4455-8ced-c8909ab09d03",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d3ab767-c65c-4fd9-b469-44edbce9585d",
   "metadata": {},
   "source": [
    "class OB_05Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OB_05Model, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers with the corrected input size\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 4096),  # Corrected input size\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(1000, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eecfe99a-b4a5-4144-87a5-5e9f2cce8f67",
   "metadata": {},
   "source": [
    "model = OB_05Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "dummy_input = torch.randn(32, 3, 64, 64)  # Batch size of 32, 3 channels, 64x64 image\n",
    "output = model(dummy_input)\n",
    "print(output.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee9e531-056f-44c2-be22-9b5fece38f56",
   "metadata": {},
   "source": [
    "Test valution with 20 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "372aaa23-5696-420c-aff7-69519c3d9ad5",
   "metadata": {},
   "source": [
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct_epoch = 0\n",
    "    total_epoch = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        # Backprop and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Train accuracy for the current batch\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        correct_epoch += correct\n",
    "        total_epoch += total\n",
    "        acc_list.append(correct / total)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Batch Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), \n",
    "                          (correct / total) * 100))\n",
    "    \n",
    "    # Epoch level accuracy\n",
    "    epoch_accuracy = 100 * correct_epoch / total_epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "print('Training finished.')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "879c3842-3fb8-4cc6-b458-4da2c200d77c",
   "metadata": {},
   "source": [
    "Use the Test Data and do the evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "928b5b4b-d020-4156-91e2-0da6d9526e0c",
   "metadata": {},
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to track correct predictions and total samples\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Update total number of labels\n",
    "        total += labels.size(0)\n",
    "        # Update the number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = (correct / total) * 100\n",
    "print('Test Accuracy of the model on  test images: {:.2f} %'.format(accuracy))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f3b33833-8bb8-4553-9a42-bc5e403a17d7",
   "metadata": {},
   "source": [
    "this for the valution test and it evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb0fbf6-1fe7-451b-a10e-714955e64a9d",
   "metadata": {},
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to track correct predictions and total samples\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Update total number of labels\n",
    "        total += labels.size(0)\n",
    "        # Update the number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = (correct / total) * 100\n",
    "print('Validation Accuracy of the model: {:.2f} %'.format(accuracy))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7ced34ad-1fd7-46d8-a3f2-c85bd10ef188",
   "metadata": {},
   "source": [
    "To Save A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91a1f753-9ee0-46ab-838c-c5593c1f39c4",
   "metadata": {},
   "source": [
    "# Define the path and file name to save the model\n",
    "model_path = r\"C:\\Users\\yasse\\OneDrive\\Desktop\\ModelPath\\model.pth\"\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f15a9-f620-4c0f-9337-4540b5a41ca2",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5879f6f-1e85-438a-a907-1d8df82f2dee",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
