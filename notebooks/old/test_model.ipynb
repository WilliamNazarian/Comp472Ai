{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation/Testing",
   "id": "2df2438a722263e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import scripts.data_loader as data_loader\n",
    "import src.training as training\n",
    "import src.evaluation as evaluation\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from src.types import *\n",
    "from src.utils.confusion_matrix import ConfusionMatrix\n",
    "from src.models.main_model import OB_05Model\n",
    "from scripts.visualization.model_evaluation import TestingVisualizations, TrainingVisualizations\n",
    "import logging\n",
    "\n",
    "cm = ConfusionMatrix\n",
    "cm_macro = ConfusionMatrix.Macro\n",
    "cm_micro = ConfusionMatrix.Micro"
   ],
   "id": "6b0860b9f59e80b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Deserializing/Loading the training logger",
   "id": "1c9d675de59bb2cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"../output/something/training_logger.pkl\", \"rb\") as file:\n",
    "    training_logger = pickle.load(file)\n",
    "\n",
    "# print(training_logger)"
   ],
   "id": "c3f5adb75cae1126",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loading the test dataset then creating a data loader from it",
   "id": "c4d11fd25d815fc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loaded_testing_dataset = torch.load('../output/something/testing_dataset.pth')\n",
    "testing_set_loader = data_loader.create_data_loader(loaded_testing_dataset)\n",
    "\n",
    "print(next(iter(loaded_testing_dataset)))"
   ],
   "id": "cf50f9d9535520b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = OB_05Model()\n",
    "model.load_state_dict(torch.load('../output/something/best_model.pth'))\n",
    "evaluation_results = evaluation.evaluate_model(logging.getLogger(), model, testing_set_loader)"
   ],
   "id": "996d561d2aa004cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Macro/Micro metrics",
   "id": "a907883ceb9690b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "confusion_matrix = evaluation_result.confusion_matrix\n",
    "\n",
    "macro_precision, macro_recall, macro_f1_score, macro_accuracy = cm_macro.calculate_overall_metrics(confusion_matrix)\n",
    "micro_precision, micro_recall, micro_f1_score, micro_accuracy = cm_micro.calculate_overall_metrics(confusion_matrix)\n",
    "accuracy = (macro_accuracy + micro_accuracy) / 2  # should be the same for both\n",
    "\n",
    "data = [[macro_precision, macro_recall, macro_f1_score, micro_precision, micro_recall, micro_f1_score, accuracy]]\n",
    "tuples = [(\"macro\", \"precision\"), (\"macro\", \"recall\"), (\"macro\", \"f1_score\"), (\"micro\", \"precision\"), (\"micro\", \"recall\"), (\"micro\", \"f1_score\"), (\"\", \"accuracy\")]\n",
    "\n",
    "df = pd.DataFrame(data,\n",
    "                  index=pd.Index([\"model\"]),\n",
    "                  columns=pd.MultiIndex.from_tuples(tuples, names=[\"\", \"metrics\"]))\n",
    "\n",
    "df.style\n",
    "\"\"\"\n",
    "\n",
    "fig = TestingVisualizations.generate_overall_metrics_table(evaluation_results)\n",
    "fig"
   ],
   "id": "5dd06e8014a7c38b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "confusion_matrix_df = EvaluationResults.get_confusion_matrix_as_df(evaluation_results)\n",
    "print(tabulate(confusion_matrix_df, headers='keys', tablefmt='pretty'))\n",
    "\n",
    "\"\"\"\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Precision': [0.85, 0.86, 0.87],\n",
    "    'Recall': [0.80, 0.82, 0.81],\n",
    "    'F1 Score': [0.82, 0.84, 0.84],\n",
    "    'Accuracy': [0.88, 0.89, 0.88]\n",
    "}\n",
    "df = pd.DataFrame(data, index=['Model 1', 'Model 2', 'Model 3'])\n",
    "\n",
    "# Display DataFrame using tabulate\n",
    "print(tabulate(df, headers='keys', tablefmt='pretty'))\n",
    "\"\"\""
   ],
   "id": "7e03b870e442361f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Confusion matrix",
   "id": "4d97063dbe81bc49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "confusion_matrix = evaluation_results.confusion_matrix\n",
    "\n",
    "df = pd.DataFrame(confusion_matrix,\n",
    "                  index=pd.Index([\"anger\", \"engaged\", \"happy\", \"neutral\"]),\n",
    "                  columns=pd.Index([\"anger\", \"engaged\", \"happy\", \"neutral\"]))\n",
    "\n"
   ],
   "id": "a4363c05c9e7e13b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Confusion matrix analysis",
   "id": "fe02973e92c07c48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "confusion_matrix = evaluation_result.confusion_matrix\n",
    "\n",
    "precisions, recalls, f1_scores, accuracies = cm.calculate_per_class_metrics(confusion_matrix)\n",
    "array = [precisions, recalls, f1_scores, accuracies]\n",
    "\n",
    "df = pd.DataFrame(array, \n",
    "                  index=pd.Index([\"precision\", \"recall\", \"f1_score\", \"accuracy\"]),\n",
    "                  columns=pd.Index([\"anger\", \"engaged\", \"happy\", \"neutral\"]))\n",
    "# df\n",
    "array"
   ],
   "id": "bbcc128a3e0d48d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "\n",
    "from pipe import *\n",
    "\n",
    "\n",
    "def process_metrics(_data, metric_name):\n",
    "    as_df = pd.DataFrame(_data, columns=[\"score\"])\n",
    "    as_df.insert(0, \"class\", [\"anger\", \"engaged\", \"happy\", \"neutral\"])\n",
    "    as_df.insert(1, \"metric\", metric_name)\n",
    "    # as_df[metric_name] = metric_name\n",
    "    return as_df \n",
    "\n",
    "precisions, recalls, f1_scores, accuracies = cm.calculate_per_class_metrics(confusion_matrix)\n",
    "\n",
    "processed_precisions = process_metrics(precisions, \"precision\")\n",
    "processed_recalls = process_metrics(recalls, \"recall\")\n",
    "processed_f1_scores = process_metrics(f1_scores, \"f1_score\")\n",
    "processed_accuracies = process_metrics(accuracies, \"accuracy\")\n",
    "\n",
    "df = pd.concat([processed_precisions, processed_recalls, processed_f1_scores, processed_accuracies])\n",
    "df"
   ],
   "id": "a6251ef8e555da56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "g = sns.catplot(df, kind=\"bar\", x=\"class\", y=\"score\", hue=\"metric\",\n",
    "                errorbar=\"sd\", alpha=0.6, height=6)\n",
    "g.despine(left=True, bottom=True)\n",
    "g.legend.set_title(\"metric\")"
   ],
   "id": "685762f45f51b3ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Metrics per class",
   "id": "f6465844530e9aa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = TestingVisualizations.plot_metrics_per_class(evaluation_results)\n",
    "fig"
   ],
   "id": "30e33120390c932a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = TestingVisualizations.generate_confusion_matrix_table(evaluation_results)\n",
    "fig\n"
   ],
   "id": "4ba8c4a79e4e022f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pipe import *\n",
    "\n",
    "confusion_matrix = evaluation_results.confusion_matrix\n",
    "precisions, recalls, f1_scores, accuracies = cm.calculate_per_class_metrics(confusion_matrix)\n",
    "\n",
    "def floats_to_strings(_data):\n",
    "    return list(_data | select(lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "floats_to_strings(list(precisions))"
   ],
   "id": "aaacdea0321965ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "127462e20c7ed3ba",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
